{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoostCredit ETL Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the ETL pipeline for processing CSV and JSON data.\n",
    "\n",
    "## Pipeline Flow:\n",
    "1. **Extract** → Read data from CSV/JSON files\n",
    "2. **Transform** → Clean, convert types, mask PII\n",
    "3. **Store** → Save to object store (Parquet)\n",
    "4. **Load** → Load from object store to PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Reload modules to ensure we have the latest code (important for notebooks)\n",
    "import src.loaders\n",
    "import src.pipeline\n",
    "import src.extractors\n",
    "import src.transformers\n",
    "\n",
    "\n",
    "from src.pipeline import Pipeline\n",
    "from src.extractors import CSVExtractor, JSONExtractor\n",
    "from src.transformers import CSVTransformer, JSONTransformer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['STORE_KEY'] = 'demo_data'\n",
    "os.environ['DB_TYPE'] = 'postgresql'\n",
    "os.environ['DB_HOST'] = 'localhost'\n",
    "os.environ['DB_PORT'] = '5432'\n",
    "os.environ['DB_USER'] = 'etl_user'\n",
    "os.environ['DB_PASSWORD'] = 'etl_password'\n",
    "os.environ['DB_NAME'] = 'etl_database'\n",
    "os.environ['DATA_PATH'] = './data'\n",
    "os.environ['OBJECT_STORE_PATH'] = './output'\n",
    "\n",
    "print(\"✓ Environment variables set\")\n",
    "print(\"✓ Modules reloaded and imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Pipeline\n",
    "\n",
    "The pipeline handles the complete ETL process automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "print(\"✓ Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test Individual Components\n",
    "\n",
    "Let's test each component separately to understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CSV Extractor\n",
    "csv_extractor = CSVExtractor()\n",
    "csv_file = Path('data/test.csv')\n",
    "if csv_file.exists():\n",
    "    sample_data = csv_extractor.extract(str(csv_file))\n",
    "    print(f\"✓ CSV Extracted: {len(sample_data)} rows\")\n",
    "    print(f\"  Columns: {list(sample_data.columns)}\")\n",
    "    print(f\"\\n  First row sample:\")\n",
    "    print(sample_data.head(1))\n",
    "else:\n",
    "    print(\"⚠ CSV file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CSV Transformer\n",
    "csv_transformer = CSVTransformer()\n",
    "if csv_file.exists():\n",
    "    transformed = csv_transformer.transform(sample_data.head(5))\n",
    "    print(\"✓ CSV Transformed\")\n",
    "    print(f\"  Data types converted\")\n",
    "    print(f\"  PII masked (name, address)\")\n",
    "    print(f\"\\n  Transformed sample:\")\n",
    "    print(transformed[['id', 'name', 'created_at', 'is_claimed', 'paid_amount']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JSON Extractor\n",
    "json_extractor = JSONExtractor()\n",
    "json_file = Path('data/test.json')\n",
    "if json_file.exists():\n",
    "    json_data = json_extractor.extract(str(json_file))\n",
    "    print(f\"✓ JSON Extracted: {len(json_data)} records\")\n",
    "    print(f\"\\n  First record keys: {list(json_data[0].keys())}\")\n",
    "    print(f\"  Sample user_id: {json_data[0].get('user_id', 'N/A')}\")\n",
    "else:\n",
    "    print(\"⚠ JSON file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JSON Transformer\n",
    "json_transformer = JSONTransformer()\n",
    "if json_file.exists():\n",
    "    json_transformed = json_transformer.transform(json_data[:2])  # Transform 2 records\n",
    "    print(\"✓ JSON Transformed into 3 tables:\")\n",
    "    print(f\"  - users: {len(json_transformed['users'])} rows\")\n",
    "    print(f\"  - telephone_numbers: {len(json_transformed['telephone_numbers'])} rows\")\n",
    "    print(f\"  - jobs_history: {len(json_transformed['jobs_history'])} rows\")\n",
    "    print(f\"\\n  Users sample:\")\n",
    "    print(json_transformed['users'][['user_id', 'name', 'username']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Complete Pipeline\n",
    "\n",
    "Now let's run the full pipeline for CSV processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CSV file\n",
    "if csv_file.exists():\n",
    "    os.environ['STORE_KEY'] = 'csv_demo'\n",
    "    pipeline.process_csv('test.csv')\n",
    "    print(\"✓ CSV processing completed!\")\n",
    "    print(\"  → Data extracted, transformed, saved to object store, and loaded to database\")\n",
    "else:\n",
    "    print(\"⚠ CSV file not found - skipping CSV processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process JSON File\n",
    "\n",
    "Process JSON data which creates multiple linked tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process JSON file\n",
    "if json_file.exists():\n",
    "    os.environ['STORE_KEY'] = 'json_demo'\n",
    "    pipeline.process_json('test.json')\n",
    "    print(\"✓ JSON processing completed!\")\n",
    "    print(\"  → Created 3 tables: users, telephone_numbers, jobs_history\")\n",
    "    print(\"  → All PII masked (emails, phones, national IDs, passwords)\")\n",
    "else:\n",
    "    print(\"⚠ JSON file not found - skipping JSON processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query Database Tables\n",
    "\n",
    "Verify that data was loaded into the database by querying all tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text, inspect\n",
    "\n",
    "# Get database credentials from environment or use defaults\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_port = os.getenv('DB_PORT', '5432')\n",
    "db_user = os.getenv('DB_USER', 'etl_user')\n",
    "db_password = os.getenv('DB_PASSWORD', 'etl_password')\n",
    "db_name = os.getenv('DB_NAME', 'etl_database')\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Get all tables\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "print(f\"✓ Connected to database: {db_name}\")\n",
    "print(f\"✓ Found {len(tables)} table(s):\\n\")\n",
    "\n",
    "# Show table information\n",
    "for table_name in tables:\n",
    "    with engine.connect() as conn:\n",
    "        # Get row count\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name}\"))\n",
    "        row_count = result.fetchone()[0]\n",
    "        \n",
    "        # Get column names\n",
    "        columns = [col['name'] for col in inspector.get_columns(table_name)]\n",
    "        \n",
    "        print(f\"Table: {table_name}\")\n",
    "        print(f\"  Rows: {row_count:,}\")\n",
    "        print(f\"  Columns: {', '.join(columns)}\")\n",
    "        print()\n",
    "\n",
    "# Close connection\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Data in Object Store\n",
    "\n",
    "Check what was saved to the object store (intermediate step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.storage import ObjectStore\n",
    "\n",
    "store = ObjectStore('./output')\n",
    "\n",
    "# Check CSV data in store\n",
    "csv_data = store.load('csv_demo', 'parquet')\n",
    "if csv_data is not None:\n",
    "    print(f\"✓ CSV data in object store: {len(csv_data)} rows\")\n",
    "    print(f\"  Columns: {list(csv_data.columns)}\")\n",
    "\n",
    "# Check JSON data in store\n",
    "json_data_store = store.load('json_demo', 'parquet')\n",
    "if json_data_store is not None:\n",
    "    print(f\"\\n✓ JSON data in object store:\")\n",
    "    for table_name, df in json_data_store.items():\n",
    "        print(f\"  - {table_name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup\n",
    "\n",
    "Close the pipeline to release database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.close()\n",
    "print(\"✓ Pipeline closed - database connections released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
