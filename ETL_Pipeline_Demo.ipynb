{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoostCredit ETL Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the ETL pipeline for processing CSV and JSON data.\n",
    "\n",
    "## Pipeline Flow:\n",
    "1. **Extract** → Read data from CSV/JSON files\n",
    "2. **Transform** → Clean, convert types, mask PII\n",
    "3. **Store** → Save to object store (Parquet)\n",
    "4. **Load** → Load from object store to PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.pipeline import Pipeline\n",
    "from src.extractors import CSVExtractor, JSONExtractor\n",
    "from src.transformers import CSVTransformer, JSONTransformer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['STORE_KEY'] = 'demo_data'\n",
    "os.environ['DB_TYPE'] = 'postgresql'\n",
    "os.environ['DB_HOST'] = 'localhost'\n",
    "os.environ['DB_PORT'] = '5432'\n",
    "os.environ['DB_USER'] = 'etl_user'\n",
    "os.environ['DB_PASSWORD'] = 'etl_password'\n",
    "os.environ['DB_NAME'] = 'etl_database'\n",
    "os.environ['DATA_PATH'] = './data'\n",
    "os.environ['OBJECT_STORE_PATH'] = './output'\n",
    "\n",
    "print(\"✓ Environment variables set\")\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Pipeline\n",
    "\n",
    "The pipeline handles the complete ETL process automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "print(\"✓ Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test Individual Components\n",
    "\n",
    "Let's test each component separately to understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CSV Extractor\n",
    "csv_extractor = CSVExtractor()\n",
    "csv_file = Path('data/test.csv')\n",
    "if csv_file.exists():\n",
    "    sample_data = csv_extractor.extract(str(csv_file))\n",
    "    print(f\"✓ CSV Extracted: {len(sample_data)} rows\")\n",
    "    print(f\"  Columns: {list(sample_data.columns)}\")\n",
    "    print(f\"\\n  First row sample:\")\n",
    "    print(sample_data.head(1))\n",
    "else:\n",
    "    print(\"⚠ CSV file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CSV Transformer\n",
    "csv_transformer = CSVTransformer()\n",
    "if csv_file.exists():\n",
    "    transformed = csv_transformer.transform(sample_data.head(5))\n",
    "    print(\"✓ CSV Transformed\")\n",
    "    print(f\"  Data types converted\")\n",
    "    print(f\"  PII masked (name, address)\")\n",
    "    print(f\"\\n  Transformed sample:\")\n",
    "    print(transformed[['id', 'name', 'created_at', 'is_claimed', 'paid_amount']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JSON Extractor\n",
    "json_extractor = JSONExtractor()\n",
    "json_file = Path('data/test.json')\n",
    "if json_file.exists():\n",
    "    json_data = json_extractor.extract(str(json_file))\n",
    "    print(f\"✓ JSON Extracted: {len(json_data)} records\")\n",
    "    print(f\"\\n  First record keys: {list(json_data[0].keys())}\")\n",
    "    print(f\"  Sample user_id: {json_data[0].get('user_id', 'N/A')}\")\n",
    "else:\n",
    "    print(\"⚠ JSON file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JSON Transformer\n",
    "json_transformer = JSONTransformer()\n",
    "if json_file.exists():\n",
    "    json_transformed = json_transformer.transform(json_data[:2])  # Transform 2 records\n",
    "    print(\"✓ JSON Transformed into 3 tables:\")\n",
    "    print(f\"  - users: {len(json_transformed['users'])} rows\")\n",
    "    print(f\"  - telephone_numbers: {len(json_transformed['telephone_numbers'])} rows\")\n",
    "    print(f\"  - jobs_history: {len(json_transformed['jobs_history'])} rows\")\n",
    "    print(f\"\\n  Users sample:\")\n",
    "    print(json_transformed['users'][['user_id', 'name', 'username']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Complete Pipeline\n",
    "\n",
    "Now let's run the full pipeline for CSV processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CSV file\n",
    "if csv_file.exists():\n",
    "    os.environ['STORE_KEY'] = 'csv_demo'\n",
    "    pipeline.process_csv('test.csv')\n",
    "    print(\"✓ CSV processing completed!\")\n",
    "    print(\"  → Data extracted, transformed, saved to object store, and loaded to database\")\n",
    "else:\n",
    "    print(\"⚠ CSV file not found - skipping CSV processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process JSON File\n",
    "\n",
    "Process JSON data which creates multiple linked tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process JSON file\n",
    "if json_file.exists():\n",
    "    os.environ['STORE_KEY'] = 'json_demo'\n",
    "    pipeline.process_json('test.json')\n",
    "    print(\"✓ JSON processing completed!\")\n",
    "    print(\"  → Created 3 tables: users, telephone_numbers, jobs_history\")\n",
    "    print(\"  → All PII masked (emails, phones, national IDs, passwords)\")\n",
    "else:\n",
    "    print(\"⚠ JSON file not found - skipping JSON processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Data in Object Store\n",
    "\n",
    "Check what was saved to the object store (intermediate step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.storage import ObjectStore\n",
    "\n",
    "store = ObjectStore('./output')\n",
    "\n",
    "# Check CSV data in store\n",
    "csv_data = store.load('csv_demo', 'parquet')\n",
    "if csv_data is not None:\n",
    "    print(f\"✓ CSV data in object store: {len(csv_data)} rows\")\n",
    "    print(f\"  Columns: {list(csv_data.columns)}\")\n",
    "\n",
    "# Check JSON data in store\n",
    "json_data_store = store.load('json_demo', 'parquet')\n",
    "if json_data_store is not None:\n",
    "    print(f\"\\n✓ JSON data in object store:\")\n",
    "    for table_name, df in json_data_store.items():\n",
    "        print(f\"  - {table_name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cleanup\n",
    "\n",
    "Close the pipeline to release database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.close()\n",
    "print(\"✓ Pipeline closed - database connections released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoostCredit ETL Pipeline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ETLPipeline' from 'src.pipeline' (/home/anser/Downloads/anserGithub/BoostCredit/src/pipeline.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49411/4140275679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mETLPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize pipeline with SQLite database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdb_connection_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sqlite:///etl_database.db'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mETLPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_connection_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ETLPipeline' from 'src.pipeline' (/home/anser/Downloads/anserGithub/BoostCredit/src/pipeline.py)"
     ]
    }
   ],
   "source": [
    "from src.pipeline import ETLPipeline\n",
    "\n",
    "# Initialize pipeline with SQLite database\n",
    "db_connection_string = 'sqlite:///etl_database.db'\n",
    "pipeline = ETLPipeline(db_connection_string)\n",
    "\n",
    "print(f\"✓ ETL Pipeline initialized\")\n",
    "print(f\"✓ Database: {db_connection_string}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exists: True\n",
      "CSV file size: 465.68 MB\n",
      "\n",
      "First 5 rows:\n",
      "     id              name                                            address  \\\n",
      "0  6311    Jennifer Green  7593 Juan Throughway Apt. 948\\nWest Corey, TX ...   \n",
      "1  3350      Karen Grimes    60975 Jessica Squares\\nEast Sallybury, FL 71671   \n",
      "2  9031       Calvin Cook                   PSC 3989, Box 4719\\nAPO AA 42056   \n",
      "3  1131    Peter Mcdowell                   PSC 1868, Box 4833\\nAPO AP 77807   \n",
      "4  1889  Mr. Ryan Sanchez      352 Simmons Circle\\nPort Dustinbury, OK 83627   \n",
      "\n",
      "    color               created_at  last_login is_claimed  paid_amount  \n",
      "0    lime  Monday, June 30th, 2013  1202190735       True  5004.671532  \n",
      "1    lime  Monday, June 30th, 2013   195884769       True   893.404595  \n",
      "2  silver           1986-06-23TEST   623477862       True   266.600000  \n",
      "3    aqua           1998-07-17TEST  1244885561       True   674.544127  \n",
      "4   white      2006-05-09 13:29:58  1293151276      truee          NaN  \n",
      "\n",
      "Columns: ['id', 'name', 'address', 'color', 'created_at', 'last_login', 'is_claimed', 'paid_amount']\n"
     ]
    }
   ],
   "source": [
    "# Check CSV file\n",
    "csv_file = Path('data/test.csv')\n",
    "print(f\"CSV file exists: {csv_file.exists()}\")\n",
    "print(f\"CSV file size: {csv_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Preview first few rows\n",
    "if csv_file.exists():\n",
    "    df_preview = pd.read_csv(csv_file, nrows=5)\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_preview)\n",
    "    print(f\"\\nColumns: {list(df_preview.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>color</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_login</th>\n",
       "      <th>is_claimed</th>\n",
       "      <th>paid_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6311</td>\n",
       "      <td>Jennifer Green</td>\n",
       "      <td>7593 Juan Throughway Apt. 948\\nWest Corey, TX ...</td>\n",
       "      <td>lime</td>\n",
       "      <td>Monday, June 30th, 2013</td>\n",
       "      <td>1202190735</td>\n",
       "      <td>True</td>\n",
       "      <td>5004.671532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3350</td>\n",
       "      <td>Karen Grimes</td>\n",
       "      <td>60975 Jessica Squares\\nEast Sallybury, FL 71671</td>\n",
       "      <td>lime</td>\n",
       "      <td>Monday, June 30th, 2013</td>\n",
       "      <td>195884769</td>\n",
       "      <td>True</td>\n",
       "      <td>893.404595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9031</td>\n",
       "      <td>Calvin Cook</td>\n",
       "      <td>PSC 3989, Box 4719\\nAPO AA 42056</td>\n",
       "      <td>silver</td>\n",
       "      <td>1986-06-23TEST</td>\n",
       "      <td>623477862</td>\n",
       "      <td>True</td>\n",
       "      <td>266.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1131</td>\n",
       "      <td>Peter Mcdowell</td>\n",
       "      <td>PSC 1868, Box 4833\\nAPO AP 77807</td>\n",
       "      <td>aqua</td>\n",
       "      <td>1998-07-17TEST</td>\n",
       "      <td>1244885561</td>\n",
       "      <td>True</td>\n",
       "      <td>674.544127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889</td>\n",
       "      <td>Mr. Ryan Sanchez</td>\n",
       "      <td>352 Simmons Circle\\nPort Dustinbury, OK 83627</td>\n",
       "      <td>white</td>\n",
       "      <td>2006-05-09 13:29:58</td>\n",
       "      <td>1293151276</td>\n",
       "      <td>truee</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id              name                                            address  \\\n",
       "0  6311    Jennifer Green  7593 Juan Throughway Apt. 948\\nWest Corey, TX ...   \n",
       "1  3350      Karen Grimes    60975 Jessica Squares\\nEast Sallybury, FL 71671   \n",
       "2  9031       Calvin Cook                   PSC 3989, Box 4719\\nAPO AA 42056   \n",
       "3  1131    Peter Mcdowell                   PSC 1868, Box 4833\\nAPO AP 77807   \n",
       "4  1889  Mr. Ryan Sanchez      352 Simmons Circle\\nPort Dustinbury, OK 83627   \n",
       "\n",
       "    color               created_at  last_login is_claimed  paid_amount  \n",
       "0    lime  Monday, June 30th, 2013  1202190735       True  5004.671532  \n",
       "1    lime  Monday, June 30th, 2013   195884769       True   893.404595  \n",
       "2  silver           1986-06-23TEST   623477862       True   266.600000  \n",
       "3    aqua           1998-07-17TEST  1244885561       True   674.544127  \n",
       "4   white      2006-05-09 13:29:58  1293151276      truee          NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CSV file\n",
    "print(\"Processing CSV file...\")\n",
    "pipeline.process_csv(str(csv_file), 'test')\n",
    "print(\"✓ CSV processing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and query test table\n",
    "engine = create_engine(db_connection_string)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Get row count\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) as count FROM test\"))\n",
    "    row_count = result.fetchone()[0]\n",
    "    print(f\"Total rows in 'test' table: {row_count}\")\n",
    "    \n",
    "    # Get sample data\n",
    "    result = conn.execute(text(\"SELECT * FROM test LIMIT 5\"))\n",
    "    columns = result.keys()\n",
    "    rows = result.fetchall()\n",
    "    \n",
    "    print(\"\\nSample data from 'test' table:\")\n",
    "    df_test = pd.DataFrame(rows, columns=columns)\n",
    "    print(df_test)\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\nData types:\")\n",
    "    result = conn.execute(text(\"PRAGMA table_info(test)\"))\n",
    "    schema = result.fetchall()\n",
    "    for col in schema:\n",
    "        print(f\"  {col[1]}: {col[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8338/3075585581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"JSON file exists: {json_file.exists()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"JSON file size: {json_file.stat().st_size / (1024*1024):.2f} MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Check JSON file\n",
    "json_file = Path('data/test.json')\n",
    "print(f\"JSON file exists: {json_file.exists()}\")\n",
    "print(f\"JSON file size: {json_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Preview first record\n",
    "if json_file.exists():\n",
    "    import json\n",
    "    with open(json_file, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        first_record = json.loads(first_line)\n",
    "        print(\"\\nFirst record structure:\")\n",
    "        print(json.dumps(first_record, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process JSON file\n",
    "print(\"Processing JSON file...\")\n",
    "pipeline.process_json(str(json_file))\n",
    "print(\"✓ JSON processing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify JSON Data in Database\n",
    "\n",
    "Let's verify that the three tables (`users`, `telephone_numbers`, `jobs_history`) were created and populated correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify users table\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) as count FROM users\"))\n",
    "    user_count = result.fetchone()[0]\n",
    "    print(f\"Total users: {user_count}\")\n",
    "    \n",
    "    result = conn.execute(text(\"SELECT * FROM users LIMIT 3\"))\n",
    "    columns = result.keys()\n",
    "    rows = result.fetchall()\n",
    "    df_users = pd.DataFrame(rows, columns=columns)\n",
    "    print(\"\\nSample users (with PII masked):\")\n",
    "    print(df_users[['user_id', 'name', 'username', 'national_id']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify telephone_numbers table\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) as count FROM telephone_numbers\"))\n",
    "    tel_count = result.fetchone()[0]\n",
    "    print(f\"Total telephone numbers: {tel_count}\")\n",
    "    \n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT tn.*, u.name \n",
    "        FROM telephone_numbers tn\n",
    "        JOIN users u ON tn.user_id = u.user_id\n",
    "        LIMIT 5\n",
    "    \"\"\"))\n",
    "    columns = result.keys()\n",
    "    rows = result.fetchall()\n",
    "    df_tel = pd.DataFrame(rows, columns=columns)\n",
    "    print(\"\\nSample telephone numbers (with PII masked):\")\n",
    "    print(df_tel[['user_id', 'name', 'telephone_number']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify jobs_history table\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) as count FROM jobs_history\"))\n",
    "    job_count = result.fetchone()[0]\n",
    "    print(f\"Total job history records: {job_count}\")\n",
    "    \n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT jh.*, u.name \n",
    "        FROM jobs_history jh\n",
    "        JOIN users u ON jh.user_id = u.user_id\n",
    "        LIMIT 5\n",
    "    \"\"\"))\n",
    "    columns = result.keys()\n",
    "    rows = result.fetchall()\n",
    "    df_jobs = pd.DataFrame(rows, columns=columns)\n",
    "    print(\"\\nSample job history:\")\n",
    "    print(df_jobs[['user_id', 'name', 'occupation', 'start', 'end', 'is_fulltime']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Foreign Key Relationships\n",
    "\n",
    "Let's verify that the foreign key relationships are working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify relationships\n",
    "with engine.connect() as conn:\n",
    "    # Check users with telephone numbers\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT \n",
    "            u.user_id,\n",
    "            u.name,\n",
    "            COUNT(tn.telephone_number) as phone_count,\n",
    "            COUNT(jh.job_id) as job_count\n",
    "        FROM users u\n",
    "        LEFT JOIN telephone_numbers tn ON u.user_id = tn.user_id\n",
    "        LEFT JOIN jobs_history jh ON u.user_id = jh.user_id\n",
    "        GROUP BY u.user_id, u.name\n",
    "        LIMIT 10\n",
    "    \"\"\"))\n",
    "    columns = result.keys()\n",
    "    rows = result.fetchall()\n",
    "    df_relationships = pd.DataFrame(rows, columns=columns)\n",
    "    print(\"Users with their telephone numbers and job counts:\")\n",
    "    print(df_relationships.to_string())\n",
    "    \n",
    "    print(\"\\n✓ Foreign key relationships verified!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Summary Statistics\n",
    "\n",
    "Let's get a summary of all the data loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "with engine.connect() as conn:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ETL Pipeline Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test table\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM test\"))\n",
    "    test_count = result.fetchone()[0]\n",
    "    print(f\"\\n✓ Test table: {test_count:,} rows\")\n",
    "    \n",
    "    # Users table\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM users\"))\n",
    "    users_count = result.fetchone()[0]\n",
    "    print(f\"✓ Users table: {users_count:,} rows\")\n",
    "    \n",
    "    # Telephone numbers\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM telephone_numbers\"))\n",
    "    tel_count = result.fetchone()[0]\n",
    "    print(f\"✓ Telephone numbers table: {tel_count:,} rows\")\n",
    "    \n",
    "    # Jobs history\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM jobs_history\"))\n",
    "    jobs_count = result.fetchone()[0]\n",
    "    print(f\"✓ Jobs history table: {jobs_count:,} rows\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ All data successfully loaded and verified!\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close pipeline\n",
    "pipeline.close()\n",
    "print(\"✓ Pipeline closed successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
